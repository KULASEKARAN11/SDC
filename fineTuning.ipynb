{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyMP7TNvXXNjUP6nK8BY3ihZ"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"widgets":{"application/vnd.jupyter.widget-state+json":{"fb6ab98c31a947a38a5ccb7e56fdc06b":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_4238f12130564ee180187357d1a08653","IPY_MODEL_ce54ef0668654e72a4e847f6bb15f778","IPY_MODEL_a56c158da124456b9fa7002ea2c6d374"],"layout":"IPY_MODEL_fd21245d4c3845329ea27a0d8021484d"}},"4238f12130564ee180187357d1a08653":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_7244896aec544205be0d3dce886209ed","placeholder":"​","style":"IPY_MODEL_51c0232bf55a44d98956bc46bbaf96f3","value":"Generating train split: "}},"ce54ef0668654e72a4e847f6bb15f778":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_a0ec150cb1d84b9283671e2d15490b48","max":1,"min":0,"orientation":"horizontal","style":"IPY_MODEL_5060c44b258b4e67b3d8d5d17ab16136","value":1}},"a56c158da124456b9fa7002ea2c6d374":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_71dfe1fe13174347907545a8a7374cc7","placeholder":"​","style":"IPY_MODEL_9f4b40ca22b34173a00ff97541650920","value":" 520/0 [00:00&lt;00:00, 20387.92 examples/s]"}},"fd21245d4c3845329ea27a0d8021484d":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"7244896aec544205be0d3dce886209ed":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"51c0232bf55a44d98956bc46bbaf96f3":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"a0ec150cb1d84b9283671e2d15490b48":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":"20px"}},"5060c44b258b4e67b3d8d5d17ab16136":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"71dfe1fe13174347907545a8a7374cc7":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"9f4b40ca22b34173a00ff97541650920":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"399b5cdf4f004c47b2aecfd9a3fbd141":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_da75eecf165d4a8eb66cce3bae6acf89","IPY_MODEL_96d828cafef84d498d5ce6db7e1d5c5e","IPY_MODEL_bcc777ed78b94d7eb482381f7c47110b"],"layout":"IPY_MODEL_3908a577527148498d896b82ae0a91ec"}},"da75eecf165d4a8eb66cce3bae6acf89":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_545084e0e7e04d778eaec9a4d71f683b","placeholder":"​","style":"IPY_MODEL_9f75104b6c634f748deaf52090823e87","value":"Running tokenizer on dataset: 100%"}},"96d828cafef84d498d5ce6db7e1d5c5e":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_93af476971684f8988f6df6b6de7dd24","max":520,"min":0,"orientation":"horizontal","style":"IPY_MODEL_4fc33c9d68074fecbd9287374584ef7b","value":520}},"bcc777ed78b94d7eb482381f7c47110b":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_943b25be86ab475294ae4ec91979a123","placeholder":"​","style":"IPY_MODEL_8465528889d340ce9f2b24d1f35a1ba0","value":" 520/520 [00:00&lt;00:00, 5917.47 examples/s]"}},"3908a577527148498d896b82ae0a91ec":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"545084e0e7e04d778eaec9a4d71f683b":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"9f75104b6c634f748deaf52090823e87":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"93af476971684f8988f6df6b6de7dd24":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"4fc33c9d68074fecbd9287374584ef7b":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"943b25be86ab475294ae4ec91979a123":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"8465528889d340ce9f2b24d1f35a1ba0":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"6541409ee73e4243989a149f6450bb8d":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_bba08b4c303f4af1b4cd16c87b16d98c","IPY_MODEL_1eb1ec5853d9443bac4513d1c94d8f38","IPY_MODEL_3867e53e55aa401391e688a8de4d35fd"],"layout":"IPY_MODEL_7d2876c8b7e64d999ab433d69bff5a1f"}},"bba08b4c303f4af1b4cd16c87b16d98c":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_f829a2b453dd4895a43bdaf62ead7cfe","placeholder":"​","style":"IPY_MODEL_224541cf97484e4cb7b2f92d9e690130","value":"Grouping texts into chunks of 512: 100%"}},"1eb1ec5853d9443bac4513d1c94d8f38":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_5efb1dd0dbb64738a7b7d77217fa841c","max":520,"min":0,"orientation":"horizontal","style":"IPY_MODEL_ded38ab3f4e1463496a60d054752dac7","value":520}},"3867e53e55aa401391e688a8de4d35fd":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_fe206afe397c4291ab1af4591ba2d9f6","placeholder":"​","style":"IPY_MODEL_f693b0389dec44a3abd3435ed5626743","value":" 520/520 [00:00&lt;00:00, 7568.11 examples/s]"}},"7d2876c8b7e64d999ab433d69bff5a1f":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f829a2b453dd4895a43bdaf62ead7cfe":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"224541cf97484e4cb7b2f92d9e690130":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"5efb1dd0dbb64738a7b7d77217fa841c":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ded38ab3f4e1463496a60d054752dac7":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"fe206afe397c4291ab1af4591ba2d9f6":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f693b0389dec44a3abd3435ed5626743":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"cells":[{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000,"referenced_widgets":["fb6ab98c31a947a38a5ccb7e56fdc06b","4238f12130564ee180187357d1a08653","ce54ef0668654e72a4e847f6bb15f778","a56c158da124456b9fa7002ea2c6d374","fd21245d4c3845329ea27a0d8021484d","7244896aec544205be0d3dce886209ed","51c0232bf55a44d98956bc46bbaf96f3","a0ec150cb1d84b9283671e2d15490b48","5060c44b258b4e67b3d8d5d17ab16136","71dfe1fe13174347907545a8a7374cc7","9f4b40ca22b34173a00ff97541650920","399b5cdf4f004c47b2aecfd9a3fbd141","da75eecf165d4a8eb66cce3bae6acf89","96d828cafef84d498d5ce6db7e1d5c5e","bcc777ed78b94d7eb482381f7c47110b","3908a577527148498d896b82ae0a91ec","545084e0e7e04d778eaec9a4d71f683b","9f75104b6c634f748deaf52090823e87","93af476971684f8988f6df6b6de7dd24","4fc33c9d68074fecbd9287374584ef7b","943b25be86ab475294ae4ec91979a123","8465528889d340ce9f2b24d1f35a1ba0","6541409ee73e4243989a149f6450bb8d","bba08b4c303f4af1b4cd16c87b16d98c","1eb1ec5853d9443bac4513d1c94d8f38","3867e53e55aa401391e688a8de4d35fd","7d2876c8b7e64d999ab433d69bff5a1f","f829a2b453dd4895a43bdaf62ead7cfe","224541cf97484e4cb7b2f92d9e690130","5efb1dd0dbb64738a7b7d77217fa841c","ded38ab3f4e1463496a60d054752dac7","fe206afe397c4291ab1af4591ba2d9f6","f693b0389dec44a3abd3435ed5626743"]},"id":"Y_OcEzdZW9On","executionInfo":{"status":"ok","timestamp":1745994766764,"user_tz":-330,"elapsed":165083,"user":{"displayName":"Kulasekaran M","userId":"01856244659462300957"}},"outputId":"6cf0a99b-82c8-4ec3-aeca-d19da1ad482f"},"outputs":[{"output_type":"stream","name":"stdout","text":["Installing necessary libraries...\n","Checking GPU availability...\n","⚠️ GPU not available, using CPU. Training will be significantly slower.\n","------------------------------\n","Configuring parameters...\n","Using model: gpt2\n","Output directory: ./gpt2-finetuned-custom\n","Training epochs: 1\n","Batch size: 4\n","Learning rate: 5e-05\n","Use FP16: False\n","Block size: 512\n","------------------------------\n","Preparing dataset...\n","Using dummy dataset (Quantum Computing)...\n","Dummy data written to dummy_custom_data.txt\n"]},{"output_type":"display_data","data":{"text/plain":["Generating train split: 0 examples [00:00, ? examples/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fb6ab98c31a947a38a5ccb7e56fdc06b"}},"metadata":{}},{"output_type":"stream","name":"stdout","text":["\n","Dataset loaded successfully:\n","DatasetDict({\n","    train: Dataset({\n","        features: ['text'],\n","        num_rows: 520\n","    })\n","})\n","\n","Sample data (first 500 chars of first entry):\n","\n","------------------------------\n","Loading tokenizer for 'gpt2'...\n","Setting pad_token to eos_token\n","\n","Loading model 'gpt2'...\n","\n","Tokenizer and model loaded successfully.\n","Model loaded on: cpu\n","------------------------------\n","Tokenizing dataset...\n"]},{"output_type":"display_data","data":{"text/plain":["Running tokenizer on dataset:   0%|          | 0/520 [00:00<?, ? examples/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"399b5cdf4f004c47b2aecfd9a3fbd141"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Grouping texts into chunks of 512:   0%|          | 0/520 [00:00<?, ? examples/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6541409ee73e4243989a149f6450bb8d"}},"metadata":{}},{"output_type":"stream","name":"stdout","text":["\n","Tokenization and grouping complete.\n","Example of processed data:\n","{'input_ids': [21906, 284, 29082, 38589, 25, 24915, 388, 14492, 17124, 1095, 14821, 12370, 19428, 588, 2208, 9150, 290, 920, 648, 1732, 284, 1620, 2653, 602, 13, 12101, 15993, 10340, 357, 15, 393, 352, 828, 627, 9895, 460, 2152, 287, 3294, 2585, 11640, 13, 9218, 50053, 287, 29082, 38589, 25, 12442, 9150, 25, 317, 627, 2545, 460, 307, 657, 11, 352, 11, 393, 257, 6087, 286, 1111, 1566, 8630, 13, 14539, 648, 1732, 25, 4930, 393, 517, 627, 9895, 460, 307, 6692, 287, 884, 257, 835, 326, 511, 277, 689, 389, 45905, 11, 7692, 286, 262, 5253, 27259, 606, 13, 2185, 45925, 530, 11101, 16717, 262, 584, 13, 24915, 388, 15953, 25, 50088, 516, 284, 15993, 9156, 17435, 11, 777, 18510, 627, 2545, 2585, 357, 68, 13, 70, 1539, 11161, 321, 446, 8946, 11, 327, 11929, 8946, 737, 41812, 34120, 287, 29082, 38589, 25, 10707, 78, 23545, 25, 1195, 549, 896, 389, 21049, 290, 4425, 511, 14821, 1181, 2233, 284, 6142, 12213, 13, 337, 2913, 1397, 763, 23545, 318, 257, 1688, 36633, 13, 12331, 35074, 25, 29082, 8563, 389, 3716, 284, 3376, 2233, 284, 262, 645, 12, 565, 12484, 44728, 13, 3351, 282, 1799, 25, 11819, 8245, 14821, 9061, 351, 257, 1588, 1271, 286, 1029, 12, 13237, 627, 9895, 318, 2408, 13, 41995, 286, 29082, 38589, 25, 37943, 23455, 25, 3184, 8306, 17745, 284, 22636, 262, 2478, 286, 649, 23533, 13, 41657, 5800, 25, 8495, 278, 5337, 5696, 351, 2176, 6608, 13, 23919, 4867, 25, 24942, 1459, 15835, 5423, 357, 68, 13, 70, 1539, 911, 273, 338, 11862, 8, 290, 5922, 14821, 12, 26128, 45898, 13, 27871, 320, 1634, 25, 4294, 1075, 3716, 23989, 2761, 5443, 621, 15993, 9061, 13, 28809, 34440, 25, 24915, 388, 15397, 4272, 25, 317, 339, 27915, 23989, 11862, 1262, 14821, 31101, 13, 9126, 2770, 1195, 549, 896, 25, 3125, 12373, 627, 9895, 1912, 319, 1353, 2770, 6608, 13, 24915, 388, 10850, 18252, 25, 5905, 3255, 262, 16246, 286, 14821, 14492, 290, 10373, 13, 21906, 284, 29082, 38589, 25, 24915, 388, 14492, 17124, 1095, 14821, 12370, 19428, 588, 2208, 9150, 290, 920, 648, 1732, 284, 1620, 2653, 602, 13, 12101, 15993, 10340, 357, 15, 393, 352, 828, 627, 9895, 460, 2152, 287, 3294, 2585, 11640, 13, 9218, 50053, 287, 29082, 38589, 25, 12442, 9150, 25, 317, 627, 2545, 460, 307, 657, 11, 352, 11, 393, 257, 6087, 286, 1111, 1566, 8630, 13, 14539, 648, 1732, 25, 4930, 393, 517, 627, 9895, 460, 307, 6692, 287, 884, 257, 835, 326, 511, 277, 689, 389, 45905, 11, 7692, 286, 262, 5253, 27259, 606, 13, 2185, 45925, 530, 11101, 16717, 262, 584, 13, 24915, 388, 15953, 25, 50088, 516, 284, 15993, 9156, 17435, 11, 777, 18510, 627, 2545, 2585, 357, 68, 13, 70, 1539, 11161, 321, 446, 8946, 11, 327, 11929, 8946, 737, 41812, 34120, 287, 29082, 38589, 25, 10707, 78, 23545, 25, 1195, 549, 896, 389, 21049, 290, 4425, 511, 14821, 1181, 2233, 284, 6142, 12213, 13, 337, 2913, 1397, 763, 23545, 318, 257, 1688, 36633, 13, 12331, 35074, 25, 29082, 8563, 389, 3716, 284, 3376, 2233, 284, 262, 645, 12, 565], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'labels': [21906, 284, 29082, 38589, 25, 24915, 388, 14492, 17124, 1095, 14821, 12370, 19428, 588, 2208, 9150, 290, 920, 648, 1732, 284, 1620, 2653, 602, 13, 12101, 15993, 10340, 357, 15, 393, 352, 828, 627, 9895, 460, 2152, 287, 3294, 2585, 11640, 13, 9218, 50053, 287, 29082, 38589, 25, 12442, 9150, 25, 317, 627, 2545, 460, 307, 657, 11, 352, 11, 393, 257, 6087, 286, 1111, 1566, 8630, 13, 14539, 648, 1732, 25, 4930, 393, 517, 627, 9895, 460, 307, 6692, 287, 884, 257, 835, 326, 511, 277, 689, 389, 45905, 11, 7692, 286, 262, 5253, 27259, 606, 13, 2185, 45925, 530, 11101, 16717, 262, 584, 13, 24915, 388, 15953, 25, 50088, 516, 284, 15993, 9156, 17435, 11, 777, 18510, 627, 2545, 2585, 357, 68, 13, 70, 1539, 11161, 321, 446, 8946, 11, 327, 11929, 8946, 737, 41812, 34120, 287, 29082, 38589, 25, 10707, 78, 23545, 25, 1195, 549, 896, 389, 21049, 290, 4425, 511, 14821, 1181, 2233, 284, 6142, 12213, 13, 337, 2913, 1397, 763, 23545, 318, 257, 1688, 36633, 13, 12331, 35074, 25, 29082, 8563, 389, 3716, 284, 3376, 2233, 284, 262, 645, 12, 565, 12484, 44728, 13, 3351, 282, 1799, 25, 11819, 8245, 14821, 9061, 351, 257, 1588, 1271, 286, 1029, 12, 13237, 627, 9895, 318, 2408, 13, 41995, 286, 29082, 38589, 25, 37943, 23455, 25, 3184, 8306, 17745, 284, 22636, 262, 2478, 286, 649, 23533, 13, 41657, 5800, 25, 8495, 278, 5337, 5696, 351, 2176, 6608, 13, 23919, 4867, 25, 24942, 1459, 15835, 5423, 357, 68, 13, 70, 1539, 911, 273, 338, 11862, 8, 290, 5922, 14821, 12, 26128, 45898, 13, 27871, 320, 1634, 25, 4294, 1075, 3716, 23989, 2761, 5443, 621, 15993, 9061, 13, 28809, 34440, 25, 24915, 388, 15397, 4272, 25, 317, 339, 27915, 23989, 11862, 1262, 14821, 31101, 13, 9126, 2770, 1195, 549, 896, 25, 3125, 12373, 627, 9895, 1912, 319, 1353, 2770, 6608, 13, 24915, 388, 10850, 18252, 25, 5905, 3255, 262, 16246, 286, 14821, 14492, 290, 10373, 13, 21906, 284, 29082, 38589, 25, 24915, 388, 14492, 17124, 1095, 14821, 12370, 19428, 588, 2208, 9150, 290, 920, 648, 1732, 284, 1620, 2653, 602, 13, 12101, 15993, 10340, 357, 15, 393, 352, 828, 627, 9895, 460, 2152, 287, 3294, 2585, 11640, 13, 9218, 50053, 287, 29082, 38589, 25, 12442, 9150, 25, 317, 627, 2545, 460, 307, 657, 11, 352, 11, 393, 257, 6087, 286, 1111, 1566, 8630, 13, 14539, 648, 1732, 25, 4930, 393, 517, 627, 9895, 460, 307, 6692, 287, 884, 257, 835, 326, 511, 277, 689, 389, 45905, 11, 7692, 286, 262, 5253, 27259, 606, 13, 2185, 45925, 530, 11101, 16717, 262, 584, 13, 24915, 388, 15953, 25, 50088, 516, 284, 15993, 9156, 17435, 11, 777, 18510, 627, 2545, 2585, 357, 68, 13, 70, 1539, 11161, 321, 446, 8946, 11, 327, 11929, 8946, 737, 41812, 34120, 287, 29082, 38589, 25, 10707, 78, 23545, 25, 1195, 549, 896, 389, 21049, 290, 4425, 511, 14821, 1181, 2233, 284, 6142, 12213, 13, 337, 2913, 1397, 763, 23545, 318, 257, 1688, 36633, 13, 12331, 35074, 25, 29082, 8563, 389, 3716, 284, 3376, 2233, 284, 262, 645, 12, 565]}\n","\n","Data collator configured for Causal LM.\n","------------------------------\n","Setting up Training Arguments...\n","Training Arguments configured.\n","------------------------------\n","Initializing Trainer...\n","Trainer initialized. Starting fine-tuning...\n","Training for 1 epochs...\n"]},{"output_type":"stream","name":"stderr","text":["<ipython-input-3-fc207c769163>:283: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n","  trainer = Trainer(\n","`loss_type=None` was set in the config but it is unrecognised.Using the default loss: `ForCausalLMLoss`.\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","    <div>\n","      \n","      <progress value='3' max='3' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [3/3 01:30, Epoch 1/1]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Step</th>\n","      <th>Training Loss</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","  </tbody>\n","</table><p>"]},"metadata":{}},{"output_type":"stream","name":"stdout","text":["\n","✅ Fine-tuning finished successfully!\n","***** train metrics *****\n","  epoch                    =        1.0\n","  total_flos               =     2920GF\n","  train_loss               =     2.4492\n","  train_runtime            = 0:02:15.18\n","  train_samples            =         12\n","  train_samples_per_second =      0.089\n","  train_steps_per_second   =      0.022\n","------------------------------\n","Saving the final fine-tuned model and tokenizer to: ./gpt2-finetuned-custom-final\n","Model and tokenizer saved successfully.\n","------------------------------\n","Testing model adaptation: Generating text...\n","\n","Using prompt: 'In quantum computing, entanglement describes the phenomenon where'\n","Generation parameters: {'max_new_tokens': 70, 'num_return_sequences': 2, 'temperature': 0.7, 'top_k': 50, 'top_p': 0.9, 'do_sample': True, 'pad_token_id': 50256}\n","\n","Loading original 'gpt2' pipeline...\n"]},{"output_type":"stream","name":"stderr","text":["Device set to use cpu\n"]},{"output_type":"stream","name":"stdout","text":["Original model pipeline loaded.\n","\n","Loading fine-tuned model pipeline from './gpt2-finetuned-custom-final'...\n"]},{"output_type":"stream","name":"stderr","text":["Device set to use cpu\n"]},{"output_type":"stream","name":"stdout","text":["Fine-tuned model pipeline loaded.\n","\n","--- Generating with ORIGINAL model ---\n","1: In quantum computing, entanglement describes the phenomenon where a photon, a photon with many particles, is entangled with a black hole.\n","\n","The entanglement can be represented by two different types of entangled particles: a black hole and a quantum field.\n","\n","The black hole is the most dense of the particles and is the only one with any energy.\n","\n","The quantum field is the most\n","2: In quantum computing, entanglement describes the phenomenon where one state of the system is in a state that is not in any other state. In quantum computation, this is called quantum entanglement.\n","\n","For quantum computing, entanglement is a state that is not in any other state.\n","\n","For quantum computation, this is called quantum entanglement.\n","\n","Quantum entanglement\n","\n","--- Generating with FINE-TUNED model ---\n","1: In quantum computing, entanglement describes the phenomenon where a system of particles is entangled by an entangled state. The entanglement occurs when a system is entangled by a set of entangled states, i.e., a set of states of a system. The entangled state is the quantum state of the system, and the entanglement is the entangled state of the system.\n","\n","The\n","2: In quantum computing, entanglement describes the phenomenon where two qubits interact with one another, and they are entangled, which means that the two qubits can communicate by quantum computing.\n","\n","In this paper, we show that the quantum computing of an entangled qubit can be done using two entangled qubits. We also demonstrate that the entangled qubits can be used to perform computations in the same\n","\n","--- Analysis ---\n","Compare the outputs above. Consider:\n","  - Relevance: Does the fine-tuned output seem more focused on the domain of your custom data?\n","  - Terminology: Does the fine-tuned model use specific words or jargon from your dataset?\n","  - Style: Is the tone/style (e.g., formal, technical, conversational) closer to your data?\n","  - Coherence: Are the generations logical and well-structured within the domain context?\n","\n","NOTE: Significant adaptation depends heavily on the size and quality of your custom dataset,\n","      as well as the chosen hyperparameters (epochs, learning rate, etc.).\n","      Experimentation might be needed for optimal results.\n","------------------------------\n","Script finished.\n"]}],"source":["# ==============================================================================\n","# STEP 0: Setup - Install Libraries and Check GPU\n","# ==============================================================================\n","print(\"Installing necessary libraries...\")\n","!pip install -q transformers datasets accelerate torch\n","\n","import torch\n","import os\n","import math\n","from datasets import load_dataset\n","from transformers import (\n","    AutoTokenizer,\n","    AutoModelForCausalLM,\n","    TrainingArguments,\n","    Trainer,\n","    DataCollatorForLanguageModeling,\n","    pipeline\n",")\n","from google.colab import files # For file uploads\n","\n","print(\"Checking GPU availability...\")\n","if torch.cuda.is_available():\n","    print(\"✅ GPU is available!\")\n","    device = torch.device(\"cuda\")\n","    # Set fp16=True in TrainingArguments for faster training on compatible GPUs\n","    use_fp16 = True\n","else:\n","    print(\"⚠️ GPU not available, using CPU. Training will be significantly slower.\")\n","    device = torch.device(\"cpu\")\n","    use_fp16 = False\n","print(\"-\" * 30)\n","\n","# ==============================================================================\n","# STEP 1: Configuration\n","# ==============================================================================\n","print(\"Configuring parameters...\")\n","\n","# --- Model Configuration ---\n","MODEL_NAME = \"gpt2\"  # Base model: \"gpt2\", \"gpt2-medium\", \"gpt2-large\", \"gpt2-xl\"\n","                  # Choose based on your resources. Larger models need more VRAM/time.\n","\n","# --- Data Configuration ---\n","# Option 1: Use Dummy Data (included below)\n","USE_DUMMY_DATA = True # Set to False if you want to upload your own file\n","\n","# Option 2: Path to your custom data file (if USE_DUMMY_DATA = False)\n","# Ensure this file is uploaded or accessible in your Colab environment\n","CUSTOM_DATA_FILE = \"my_custom_data.txt\" # Change this if your filename is different\n","\n","# --- Training Hyperparameters ---\n","OUTPUT_DIR = \"./gpt2-finetuned-custom\"      # Where the fine-tuned model will be saved\n","NUM_TRAIN_EPOCHS = 1                      # Start with 1-3 epochs. Increase cautiously.\n","PER_DEVICE_TRAIN_BATCH_SIZE = 4           # Lower this if you encounter CUDA Out-of-Memory errors (e.g., 2 or 1)\n","LEARNING_RATE = 5e-5                      # Common starting point for fine-tuning (AdamW optimizer)\n","WARMUP_STEPS = 100                        # Number of steps for learning rate warmup\n","WEIGHT_DECAY = 0.01                       # Regularization parameter\n","LOGGING_STEPS = 100                       # How often to log training metrics (loss, etc.)\n","SAVE_STEPS = 500                          # How often to save a model checkpoint during training\n","SAVE_TOTAL_LIMIT = 2                      # Maximum number of checkpoints to keep (saves disk space)\n","\n","# --- Tokenizer Configuration ---\n","# Use model's max length or choose a smaller size if memory is limited\n","# GPT-2 max length is 1024\n","BLOCK_SIZE = 512                          # Process data in chunks of this size. Adjust based on VRAM.\n","\n","print(f\"Using model: {MODEL_NAME}\")\n","print(f\"Output directory: {OUTPUT_DIR}\")\n","print(f\"Training epochs: {NUM_TRAIN_EPOCHS}\")\n","print(f\"Batch size: {PER_DEVICE_TRAIN_BATCH_SIZE}\")\n","print(f\"Learning rate: {LEARNING_RATE}\")\n","print(f\"Use FP16: {use_fp16}\")\n","print(f\"Block size: {BLOCK_SIZE}\")\n","print(\"-\" * 30)\n","\n","# ==============================================================================\n","# STEP 2: Prepare Data\n","# ==============================================================================\n","print(\"Preparing dataset...\")\n","\n","data_file_path = None\n","\n","if USE_DUMMY_DATA:\n","    print(\"Using dummy dataset (Quantum Computing)...\")\n","    dummy_data_content = \"\"\"\n","Introduction to Quantum Computing:\n","Quantum computing leverages quantum mechanical phenomena like superposition and entanglement to perform computations. Unlike classical bits (0 or 1), qubits can exist in multiple states simultaneously.\n","\n","Key Concepts in Quantum Computing:\n","Superposition: A qubit can be 0, 1, or a combination of both until measured.\n","Entanglement: Two or more qubits can be linked in such a way that their fates are intertwined, regardless of the distance separating them. Measuring one instantly influences the other.\n","Quantum Gates: Analogous to classical logic gates, these manipulate qubit states (e.g., Hadamard gate, CNOT gate).\n","\n","Challenges in Quantum Computing:\n","Decoherence: Qubits are fragile and lose their quantum state due to environmental interactions. Maintaining coherence is a major hurdle.\n","Error Correction: Quantum errors are complex to correct due to the no-cloning theorem.\n","Scalability: Building stable quantum computers with a large number of high-quality qubits is difficult.\n","\n","Applications of Quantum Computing:\n","Drug Discovery: Simulating molecules to accelerate the development of new medicines.\n","Materials Science: Designing novel materials with specific properties.\n","Cryptography: Breaking current encryption standards (e.g., Shor's algorithm) and developing quantum-resistant cryptography.\n","Optimization: Solving complex optimization problems faster than classical computers.\n","\n","Advanced Topics:\n","Quantum Annealing: A heuristic optimization algorithm using quantum fluctuations.\n","Topological Qubits: More robust qubits based on topological properties.\n","Quantum Machine Learning: Exploring the intersection of quantum computing and ML.\n","\"\"\"\n","    data_file_path = \"dummy_custom_data.txt\"\n","    # Repeat the dummy data to make the dataset slightly larger for demonstration\n","    with open(data_file_path, \"w\") as f:\n","        for _ in range(20): # Write the content 20 times\n","             f.write(dummy_data_content + \"\\n\\n\") # Add extra newline for separation\n","    print(f\"Dummy data written to {data_file_path}\")\n","\n","else:\n","    print(f\"Attempting to use custom data file: {CUSTOM_DATA_FILE}\")\n","    if os.path.exists(CUSTOM_DATA_FILE):\n","        print(f\"Found existing file: {CUSTOM_DATA_FILE}\")\n","        data_file_path = CUSTOM_DATA_FILE\n","    else:\n","        print(f\"File '{CUSTOM_DATA_FILE}' not found. Please upload your .txt dataset file.\")\n","        try:\n","            uploaded = files.upload()\n","            if not uploaded:\n","                raise ValueError(\"No file uploaded.\")\n","            # Get the first uploaded file name\n","            uploaded_filename = list(uploaded.keys())[0]\n","            # Rename it to CUSTOM_DATA_FILE if necessary, or just use the uploaded name\n","            if uploaded_filename != CUSTOM_DATA_FILE:\n","                 print(f\"Uploaded file '{uploaded_filename}', renaming to '{CUSTOM_DATA_FILE}' for consistency.\")\n","                 os.rename(uploaded_filename, CUSTOM_DATA_FILE)\n","\n","            data_file_path = CUSTOM_DATA_FILE\n","            print(f\"Successfully uploaded and using: {data_file_path}\")\n","        except Exception as e:\n","            print(f\"Error during file upload: {e}\")\n","            print(\"Please ensure you upload a single .txt file when prompted.\")\n","            # Stop execution if data loading fails\n","            raise SystemExit(\"Data loading failed.\")\n","\n","# --- Load the dataset ---\n","if data_file_path:\n","    try:\n","        # Load dataset using 'text' type for plain text files\n","        # Assumes one document/example per line, or treats the whole file as one long string\n","        raw_datasets = load_dataset('text', data_files={'train': data_file_path})\n","        print(\"\\nDataset loaded successfully:\")\n","        print(raw_datasets)\n","        # Display a sample\n","        print(\"\\nSample data (first 500 chars of first entry):\")\n","        print(raw_datasets['train'][0]['text'][:500])\n","    except Exception as e:\n","        print(f\"\\nError loading dataset from file {data_file_path}: {e}\")\n","        print(\"Ensure the file exists, is readable, and is a plain text (.txt) file.\")\n","        raise SystemExit(\"Dataset loading failed.\")\n","else:\n","     raise SystemExit(\"No data file path specified or found. Cannot proceed.\")\n","\n","print(\"-\" * 30)\n","\n","# ==============================================================================\n","# STEP 3: Load Tokenizer and Model\n","# ==============================================================================\n","print(f\"Loading tokenizer for '{MODEL_NAME}'...\")\n","tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n","\n","# Set pad token if it's not already set (GPT-2 usually requires this)\n","if tokenizer.pad_token is None:\n","    print(\"Setting pad_token to eos_token\")\n","    tokenizer.pad_token = tokenizer.eos_token\n","\n","print(f\"\\nLoading model '{MODEL_NAME}'...\")\n","# Load the model for Causal Language Modeling (text generation)\n","model = AutoModelForCausalLM.from_pretrained(MODEL_NAME)\n","\n","# Move model to the appropriate device (GPU or CPU)\n","model.to(device)\n","\n","# Resize token embeddings if new tokens were added (not typical for basic fine-tuning)\n","# model.resize_token_embeddings(len(tokenizer))\n","\n","print(\"\\nTokenizer and model loaded successfully.\")\n","print(f\"Model loaded on: {model.device}\")\n","print(\"-\" * 30)\n","\n","# ==============================================================================\n","# STEP 4: Tokenize and Prepare Data for Training\n","# ==============================================================================\n","print(\"Tokenizing dataset...\")\n","\n","# Tokenization function\n","def tokenize_function(examples):\n","    # This simple approach tokenizes each text individually.\n","    # Padding/truncation will be handled by the data collator.\n","     return tokenizer(examples[\"text\"],\n","                      # truncation=True, # Option 1: Truncate long examples\n","                      # max_length=BLOCK_SIZE,\n","                      add_special_tokens=True)\n","\n","# More advanced function to concatenate texts and chunk them into blocks\n","# This can be more efficient for training but requires careful handling\n","def group_texts(examples):\n","    # Concatenate all texts. Add EOS token between documents.\n","    concatenated_examples = {k: sum(examples[k], []) for k in examples.keys()}\n","    total_length = len(concatenated_examples[list(examples.keys())[0]])\n","    # We drop the small remainder, we could add padding if the model supported it instead of this drop, you can\n","    # customize this part to your needs.\n","    if total_length >= BLOCK_SIZE:\n","        total_length = (total_length // BLOCK_SIZE) * BLOCK_SIZE\n","    # Split by chunks of BLOCK_SIZE.\n","    result = {\n","        k: [t[i : i + BLOCK_SIZE] for i in range(0, total_length, BLOCK_SIZE)]\n","        for k, t in concatenated_examples.items()\n","    }\n","    # Create labels for Causal LM (predict the next token)\n","    result[\"labels\"] = result[\"input_ids\"].copy()\n","    return result\n","\n","# --- Apply tokenization ---\n","# First, tokenize individual lines/documents\n","tokenized_datasets_intermediate = raw_datasets.map(\n","    tokenize_function,\n","    batched=True,\n","    remove_columns=raw_datasets[\"train\"].column_names, # Remove original 'text' column\n","    desc=\"Running tokenizer on dataset\",\n",")\n","\n","# Then, group into blocks (optional but recommended for Causal LM)\n","tokenized_datasets = tokenized_datasets_intermediate.map(\n","    group_texts,\n","    batched=True,\n","    desc=f\"Grouping texts into chunks of {BLOCK_SIZE}\",\n",")\n","\n","print(\"\\nTokenization and grouping complete.\")\n","print(\"Example of processed data:\")\n","print(tokenized_datasets[\"train\"][0])\n","\n","# --- Data Collator ---\n","# Handles dynamic padding within batches and prepares labels for Causal LM\n","data_collator = DataCollatorForLanguageModeling(\n","    tokenizer=tokenizer,\n","    mlm=False  # False for Causal LM (GPT-2), True for Masked LM (BERT)\n",")\n","print(\"\\nData collator configured for Causal LM.\")\n","print(\"-\" * 30)\n","\n","# ==============================================================================\n","# STEP 5: Configure Training Arguments\n","# ==============================================================================\n","print(\"Setting up Training Arguments...\")\n","\n","training_args = TrainingArguments(\n","    output_dir=OUTPUT_DIR,                  # Directory to save model checkpoints and logs\n","    overwrite_output_dir=True,              # Overwrite the content of the output directory\n","    num_train_epochs=NUM_TRAIN_EPOCHS,      # Total number of training epochs\n","    per_device_train_batch_size=PER_DEVICE_TRAIN_BATCH_SIZE, # Batch size per GPU/CPU\n","    learning_rate=LEARNING_RATE,            # Initial learning rate\n","    warmup_steps=WARMUP_STEPS,              # Number of warmup steps for learning rate scheduler\n","    weight_decay=WEIGHT_DECAY,              # Strength of weight decay regularization\n","    logging_dir='./logs',                   # Directory for storing logs (e.g., TensorBoard)\n","    logging_steps=LOGGING_STEPS,            # Log training metrics every X steps\n","    save_steps=SAVE_STEPS,                  # Save a checkpoint every X steps\n","    save_total_limit=SAVE_TOTAL_LIMIT,      # Limit the total number of checkpoints saved\n","    fp16=use_fp16,                          # Enable mixed precision training if GPU is available and compatible\n","    report_to=\"none\",                       # Disable external reporting integrations (like WandB/TensorBoard) for simplicity\n","    # evaluation_strategy=\"steps\",          # Uncomment if you have an eval dataset\n","    # eval_steps=SAVE_STEPS,                # Evaluate every 'eval_steps'\n","    # per_device_eval_batch_size=PER_DEVICE_TRAIN_BATCH_SIZE * 2, # Batch size for evaluation\n","    # load_best_model_at_end=True,          # Uncomment if using evaluation to load the best model found\n","    # metric_for_best_model=\"loss\",         # Uncomment if using evaluation\n",")\n","\n","print(\"Training Arguments configured.\")\n","print(\"-\" * 30)\n","\n","# ==============================================================================\n","# STEP 6: Initialize Trainer and Start Fine-Tuning\n","# ==============================================================================\n","print(\"Initializing Trainer...\")\n","\n","trainer = Trainer(\n","    model=model,                            # The instantiated Transformers model to be trained\n","    args=training_args,                     # Training arguments, defined above\n","    train_dataset=tokenized_datasets[\"train\"], # Training dataset\n","    # eval_dataset=tokenized_datasets[\"validation\"], # Evaluation dataset (optional)\n","    tokenizer=tokenizer,                    # Tokenizer for saving purposes\n","    data_collator=data_collator,            # Data collator to handle batching and padding\n",")\n","\n","print(\"Trainer initialized. Starting fine-tuning...\")\n","print(f\"Training for {NUM_TRAIN_EPOCHS} epochs...\")\n","\n","# --- Start Training ---\n","try:\n","    train_result = trainer.train()\n","    print(\"\\n✅ Fine-tuning finished successfully!\")\n","\n","    # --- Log some metrics ---\n","    metrics = train_result.metrics\n","    metrics[\"train_samples\"] = len(tokenized_datasets[\"train\"])\n","    trainer.log_metrics(\"train\", metrics)\n","    trainer.save_metrics(\"train\", metrics)\n","\n","    # --- Calculate perplexity if evaluation was done ---\n","    # Needs an eval_dataset configured in Trainer and evaluation_strategy != \"no\"\n","    # try:\n","    #     eval_metrics = trainer.evaluate()\n","    #     perplexity = math.exp(eval_metrics[\"eval_loss\"])\n","    #     print(f\"\\nPerplexity on evaluation set: {perplexity:.2f}\")\n","    #     trainer.log_metrics(\"eval\", eval_metrics)\n","    #     trainer.save_metrics(\"eval\", eval_metrics)\n","    # except KeyError:\n","    #     print(\"\\nEvaluation metrics not found (is eval_dataset provided and evaluation_strategy set?).\")\n","    # except Exception as e:\n","    #      print(f\"\\nError during evaluation: {e}\")\n","\n","except torch.cuda.OutOfMemoryError:\n","    print(\"\\n❌ CUDA Out of Memory Error!\")\n","    print(\"Training stopped. Suggestions:\")\n","    print(f\"  - Decrease `PER_DEVICE_TRAIN_BATCH_SIZE` (current: {PER_DEVICE_TRAIN_BATCH_SIZE})\")\n","    print(f\"  - Decrease `BLOCK_SIZE` (current: {BLOCK_SIZE})\")\n","    print(f\"  - Use a smaller model from the {MODEL_NAME} family (e.g., 'gpt2' instead of 'gpt2-medium')\")\n","    print(\"  - If using Colab Pro, consider upgrading to a High-RAM runtime.\")\n","    # Clean up GPU memory\n","    del model\n","    del trainer\n","    torch.cuda.empty_cache()\n","    raise SystemExit(\"OOM Error during training.\") # Stop execution\n","\n","except Exception as e:\n","    print(f\"\\n❌ An unexpected error occurred during training: {e}\")\n","    raise SystemExit(\"Training failed.\") # Stop execution\n","\n","print(\"-\" * 30)\n","\n","# ==============================================================================\n","# STEP 7: Save the Fine-Tuned Model and Tokenizer\n","# ==============================================================================\n","final_model_path = f\"{OUTPUT_DIR}-final\"\n","print(f\"Saving the final fine-tuned model and tokenizer to: {final_model_path}\")\n","\n","try:\n","    # Save the trained model weights and configuration\n","    trainer.save_model(final_model_path)\n","    # Save the tokenizer configuration as well (important!)\n","    tokenizer.save_pretrained(final_model_path)\n","    print(\"Model and tokenizer saved successfully.\")\n","except Exception as e:\n","    print(f\"Error saving model/tokenizer: {e}\")\n","\n","print(\"-\" * 30)\n","\n","# ==============================================================================\n","# STEP 8: Test Adaptation - Generate Text Comparison\n","# ==============================================================================\n","print(\"Testing model adaptation: Generating text...\")\n","\n","# --- Crucial: Define a prompt relevant to YOUR domain ---\n","# Replace this example prompt with one related to your custom dataset!\n","if USE_DUMMY_DATA:\n","    prompt = \"In quantum computing, entanglement describes the phenomenon where\"\n","else:\n","    prompt = \"YOUR_DOMAIN_SPECIFIC_PROMPT_HERE\" # <--- CHANGE THIS !!!\n","    print(f\"⚠️ Using placeholder prompt: '{prompt}'. Please change it to be relevant to your domain '{CUSTOM_DATA_FILE}' for meaningful comparison.\")\n","\n","\n","# --- Generation Parameters ---\n","max_new_tokens = 70      # Max number of *new* tokens to generate after the prompt\n","num_sequences = 2       # Number of different sequences to generate\n","temperature = 0.7       # Controls randomness (lower = more deterministic, higher = more random)\n","top_k = 50              # Consider only the top K most likely tokens at each step\n","top_p = 0.9             # Consider only tokens whose cumulative probability is >= P (nucleus sampling)\n","do_sample = True        # Whether to use sampling; False means greedy decoding\n","\n","generation_config = {\n","    \"max_new_tokens\": max_new_tokens,\n","    \"num_return_sequences\": num_sequences,\n","    \"temperature\": temperature,\n","    \"top_k\": top_k,\n","    \"top_p\": top_p,\n","    \"do_sample\": do_sample,\n","    \"pad_token_id\": tokenizer.eos_token_id # Use EOS token for padding during generation\n","}\n","\n","print(f\"\\nUsing prompt: '{prompt}'\")\n","print(f\"Generation parameters: {generation_config}\")\n","\n","# --- Load Original Model Pipeline ---\n","generator_original = None\n","try:\n","    print(f\"\\nLoading original '{MODEL_NAME}' pipeline...\")\n","    generator_original = pipeline(\n","        'text-generation',\n","        model=MODEL_NAME,\n","        tokenizer=MODEL_NAME,\n","        device=0 if torch.cuda.is_available() else -1 # Use GPU 0 if available, else CPU\n","    )\n","    print(\"Original model pipeline loaded.\")\n","except Exception as e:\n","    print(f\"Error loading original model pipeline: {e}\")\n","\n","# --- Load Fine-tuned Model Pipeline ---\n","generator_finetuned = None\n","if os.path.exists(final_model_path):\n","    try:\n","        print(f\"\\nLoading fine-tuned model pipeline from '{final_model_path}'...\")\n","        generator_finetuned = pipeline(\n","            'text-generation',\n","            model=final_model_path,   # Path to your saved model directory\n","            tokenizer=final_model_path, # Path to your saved tokenizer directory\n","            device=0 if torch.cuda.is_available() else -1 # Use GPU 0 if available, else CPU\n","        )\n","        print(\"Fine-tuned model pipeline loaded.\")\n","    except Exception as e:\n","        print(f\"Error loading fine-tuned model pipeline: {e}\")\n","else:\n","    print(f\"Fine-tuned model path '{final_model_path}' not found. Skipping fine-tuned generation.\")\n","\n","\n","# --- Generate and Compare ---\n","print(\"\\n--- Generating with ORIGINAL model ---\")\n","if generator_original:\n","    try:\n","        outputs_original = generator_original(prompt, **generation_config)\n","        for i, output in enumerate(outputs_original):\n","            print(f\"{i+1}: {output['generated_text']}\")\n","    except Exception as e:\n","        print(f\"Error during original model generation: {e}\")\n","else:\n","    print(\"Skipping original model generation.\")\n","\n","\n","print(\"\\n--- Generating with FINE-TUNED model ---\")\n","if generator_finetuned:\n","    try:\n","        outputs_finetuned = generator_finetuned(prompt, **generation_config)\n","        for i, output in enumerate(outputs_finetuned):\n","            print(f\"{i+1}: {output['generated_text']}\")\n","    except Exception as e:\n","        print(f\"Error during fine-tuned model generation: {e}\")\n","else:\n","    print(\"Skipping fine-tuned model generation.\")\n","\n","\n","# --- Qualitative Analysis Guidance ---\n","print(\"\\n--- Analysis ---\")\n","print(\"Compare the outputs above. Consider:\")\n","print(\"  - Relevance: Does the fine-tuned output seem more focused on the domain of your custom data?\")\n","print(\"  - Terminology: Does the fine-tuned model use specific words or jargon from your dataset?\")\n","print(\"  - Style: Is the tone/style (e.g., formal, technical, conversational) closer to your data?\")\n","print(\"  - Coherence: Are the generations logical and well-structured within the domain context?\")\n","print(\"\\nNOTE: Significant adaptation depends heavily on the size and quality of your custom dataset,\")\n","print(\"      as well as the chosen hyperparameters (epochs, learning rate, etc.).\")\n","print(\"      Experimentation might be needed for optimal results.\")\n","print(\"-\" * 30)\n","print(\"Script finished.\")\n","# =============================================================================="]}]}